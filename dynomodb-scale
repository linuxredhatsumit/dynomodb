import argparse
import boto3
import time
from datetime import datetime
import logging
import json
import sys
import os
from botocore.exceptions import ClientError

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S"),
            "level": record.levelname,
            "context": getattr(record, "context", "autoscaling"),
            "message": record.getMessage()
        }
        return json.dumps(log_entry)

logger = logging.getLogger()
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

def get_table_description(dynamodb, table_name):
    try:
        return dynamodb.describe_table(TableName=table_name)['Table']
    except ClientError as e:
        logger.error(json.dumps({"context": "describe_table", "error": f"AWS Error: {e.response['Error']['Code']} - {e.response['Error']['Message']}"}))
        sys.exit(1)

def wait_for_table_active(dynamodb, table_name, target_billing_mode=None):
    max_attempts = 60
    attempt = 0
    while attempt < max_attempts:
        desc = get_table_description(dynamodb, table_name)
        current_billing_mode = desc.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')
        if desc['TableStatus'] == 'ACTIVE' and (target_billing_mode is None or current_billing_mode == target_billing_mode):
            logger.info(json.dumps({
                "context": "wait_for_table_active",
                "message": f"Table {table_name} is ACTIVE with BillingMode {current_billing_mode}"
            }))
            return
        logger.info(json.dumps({
            "context": "wait_for_table_active",
            "message": f"Waiting for table {table_name} to become ACTIVE and BillingMode {target_billing_mode}... (current: {desc['TableStatus']}, {current_billing_mode})"
        }))
        time.sleep(5)
        attempt += 1
    logger.error(json.dumps({
        "context": "wait_for_table_active",
        "error": f"Timeout waiting for table {table_name} to reach ACTIVE status and BillingMode {target_billing_mode}"
    }))
    sys.exit(1)

def get_replica_regions(dynamodb, table_name):
    try:
        response = dynamodb.list_global_tables()
        global_tables = response.get('GlobalTables', [])
        for table in global_tables:
            if table['TableName'] == table_name:
                replicas = [replica['RegionName'] for replica in table.get('Replicas', [])]
                logger.info(json.dumps({
                    "context": "global_tables",
                    "message": f"Found replica regions for {table_name}: {replicas}"
                }))
                return replicas
        logger.info(json.dumps({
            "context": "global_tables",
            "message": f"Table {table_name} is not a Global Table or has no replicas."
        }))
        return []
    except ClientError as e:
        logger.error(json.dumps({"context": "global_tables", "error": f"AWS Error: {e.response['Error']['Code']} - {e.response['Error']['Message']}"}))
        sys.exit(1)

def load_gsi_config(table_name, check_gsi):
    if not check_gsi:
        logger.info("checkGSI is false. Skipping GSI config load.", extra={"context": "config.load"})
        return {}
    repo_root = os.environ.get("Build_SourcesDirectory", ".")
    repo_name = os.environ.get("REPOSITORY_NAME", "gsi-config-repo")
    folder_path = os.path.join(repo_root, repo_name, table_name)
    file_path = os.path.join(folder_path, "scaling.json")
    logger.info(f"Looking for config file at: {file_path}", extra={"context": "config.load"})
    if not os.path.exists(folder_path):
        logger.warning(json.dumps({
            "context": "config.load",
            "message": f"Table folder not found at: {folder_path}. Assuming no GSIs."
        }))
        return {}
    if not os.path.exists(file_path):
        logger.warning(json.dumps({
            "context": "config.load",
            "message": f"Scaling config file not found at: {file_path}. Assuming no GSIs."
        }))
        return {}
    try:
        with open(file_path, "r") as f:
            gsi_config = json.load(f)
        valid_projection_types = {"ALL", "KEYS_ONLY", "INCLUDE"}
        for index_name, conf in gsi_config.items():
            if "ProjectionType" in conf and conf["ProjectionType"] not in valid_projection_types:
                logger.error(json.dumps({
                    "context": "config.validate",
                    "error": f"Invalid ProjectionType for index {index_name}: {conf['ProjectionType']}. Must be one of {valid_projection_types}"
                }))
                sys.exit(1)
            if conf.get("ProjectionType") == "INCLUDE" and "NonKeyAttributes" not in conf:
                logger.error(json.dumps({
                    "context": "config.validate",
                    "error": f"NonKeyAttributes required for index {index_name} when ProjectionType is INCLUDE"
                }))
                sys.exit(1)
            if "NonKeyAttributes" in conf and not isinstance(conf["NonKeyAttributes"], list):
                logger.error(json.dumps({
                    "context": "config.validate",
                    "error": f"NonKeyAttributes for index {index_name} must be a list"
                }))
                sys.exit(1)
        logger.info(json.dumps({"context": "config.load", "message": f"Loaded GSI config: {gsi_config}"}))
        return gsi_config
    except json.JSONDecodeError as e:
        logger.error(json.dumps({"context": "config.load", "error": f"Invalid JSON format: {str(e)}"}))
        sys.exit(1)

def validate_gsi_config(desc, gsi_config, check_gsi):
    if not check_gsi or not gsi_config:
        logger.info("No GSI config to validate (checkGSI=false or no config found).", extra={"context": "config.validate"})
        return
    actual_indexes = set(gsi['IndexName'] for gsi in desc.get("GlobalSecondaryIndexes", []))
    config_indexes = set(gsi_config.keys())
    if actual_indexes != config_indexes:
        logger.error(json.dumps({
            "context": "config.validate",
            "error": "Mismatch between actual GSIs and scaling.json",
            "actual_indexes": list(actual_indexes),
            "config_indexes": list(config_indexes)
        }))
        sys.exit(1)

def update_gsi_configuration(dynamodb, table_name, gsi_config):
    try:
        updated = False
        table_desc = dynamodb.describe_table(TableName=table_name)["Table"]
        existing_gsis = {gsi["IndexName"]: gsi for gsi in table_desc.get("GlobalSecondaryIndexes", [])}
        table_attribute_definitions = table_desc.get("AttributeDefinitions", [])
        gsi_updates = []
        for index_name, conf in gsi_config.items():
            existing = existing_gsis.get(index_name)
            if not existing:
                logger.warning(json.dumps({
                    "context": "gsi.update",
                    "message": f"GSI {index_name} not found in table. Skipping update."
                }))
                continue
            projection_needs_update = False
            throughput_needs_update = False
            current_projection = existing.get("Projection", {})
            desired_projection_type = conf.get("ProjectionType", current_projection.get("ProjectionType", "ALL"))
            desired_non_key_attributes = conf.get("NonKeyAttributes", [])
            current_non_key_attributes = current_projection.get("NonKeyAttributes", [])
            logger.info(json.dumps({
                "context": "gsi.update",
                "message": f"Checking projection attributes for GSI {index_name}",
                "current_projection_type": current_projection.get("ProjectionType"),
                "desired_projection_type": desired_projection_type,
                "current_non_key_attributes": current_non_key_attributes,
                "desired_non_key_attributes": desired_non_key_attributes
            }))
            if (current_projection.get("ProjectionType") != desired_projection_type or
                    sorted(current_non_key_attributes or []) != sorted(desired_non_key_attributes or [])):
                projection_needs_update = True
                logger.info(json.dumps({
                    "context": "gsi.update",
                    "message": f"Projection attributes for GSI {index_name} need updating: current={current_projection}, desired={{'ProjectionType': '{desired_projection_type}', 'NonKeyAttributes': {desired_non_key_attributes}}}"
                }))
            current_throughput = existing.get("ProvisionedThroughput", {})
            desired_throughput = {
                "ReadCapacityUnits": conf.get("ReadCapacityUnits", 5),
                "WriteCapacityUnits": conf.get("WriteCapacityUnits", 5)
            }
            if (current_throughput.get("ReadCapacityUnits") != desired_throughput["ReadCapacityUnits"] or
                    current_throughput.get("WriteCapacityUnits") != desired_throughput["WriteCapacityUnits"]):
                throughput_needs_update = True
                logger.info(json.dumps({
                    "context": "gsi.update",
                    "message": f"Throughput for GSI {index_name} needs updating",
                    "current_read_capacity": current_throughput.get("ReadCapacityUnits"),
                    "current_write_capacity": current_throughput.get("WriteCapacityUnits"),
                    "desired_read_capacity": desired_throughput["ReadCapacityUnits"],
                    "desired_write_capacity": desired_throughput["WriteCapacityUnits"]
                }))
            if projection_needs_update:
                logger.info(json.dumps({
                    "context": "gsi.update",
                    "message": f"Deleting GSI {index_name} to update projection attributes"
                }))
                gsi_updates.append({
                    "Delete": {
                        "IndexName": index_name
                    }
                })
                dynamodb.update_table(
                    TableName=table_name,
                    GlobalSecondaryIndexUpdates=gsi_updates
                )
                wait_for_table_active(dynamodb, table_name)
                gsi_updates = []
                key_attributes = [ks["AttributeName"] for ks in existing["KeySchema"]]
                required_attributes = set(key_attributes + desired_non_key_attributes)
                attribute_definitions = []
                attribute_types = {
                    "id": "S",
                    "status": "S",
                    "mandateStatus": "S",
                    "customerid": "N",
                    "email": "S"
                }
                for attr in required_attributes:
                    attr_type = attribute_types.get(attr)
                    if not attr_type:
                        logger.warning(json.dumps({
                            "context": "gsi.update",
                            "message": f"Attribute {attr} not found in table's AttributeDefinitions. Assuming type 'S'."
                        }))
                        attr_type = "S"
                    attribute_definitions.append({
                        "AttributeName": attr,
                        "AttributeType": attr_type
                    })
                create_update = {
                    "Create": {
                        "IndexName": index_name,
                        "KeySchema": existing["KeySchema"],
                        "Projection": {
                            "ProjectionType": desired_projection_type
                        },
                        "ProvisionedThroughput": desired_throughput
                    }
                }
                if desired_projection_type == "INCLUDE":
                    create_update["Create"]["Projection"]["NonKeyAttributes"] = desired_non_key_attributes
                gsi_updates.append(create_update)
                updated = True
            elif throughput_needs_update:
                gsi_updates.append({
                    "Update": {
                        "IndexName": index_name,
                        "ProvisionedThroughput": desired_throughput
                    }
                })
                updated = True
        if gsi_updates:
            logger.info(json.dumps({
                "context": "gsi.update",
                "message": f"Applying GSI updates: {gsi_updates}"
            }))
            update_kwargs = {
                "TableName": table_name,
                "GlobalSecondaryIndexUpdates": gsi_updates
            }
            if any("Create" in update for update in gsi_updates):
                update_kwargs["AttributeDefinitions"] = attribute_definitions
                logger.info(json.dumps({
                    "context": "gsi.update",
                    "message": f"Including AttributeDefinitions: {attribute_definitions}"
                }))
            dynamodb.update_table(**update_kwargs)
            logger.info("GSI configuration update submitted. Waiting 10 seconds before continuing...")
            time.sleep(10)
        final_desc = dynamodb.describe_table(TableName=table_name)["Table"]
        for gsi in final_desc.get("GlobalSecondaryIndexes", []):
            name = gsi["IndexName"]
            rcu = gsi["ProvisionedThroughput"]["ReadCapacityUnits"]
            wcu = gsi["ProvisionedThroughput"]["WriteCapacityUnits"]
            projection = gsi.get("Projection", {})
            logger.info(json.dumps({
                "context": "gsi.verification",
                "message": f"Final configuration for GSI {name}",
                "IndexName": name,
                "ReadCapacityUnits": rcu,
                "WriteCapacityUnits": wcu,
                "ProjectionType": projection.get("ProjectionType"),
                "NonKeyAttributes": projection.get("NonKeyAttributes", [])
            }))
        return updated
    except ClientError as e:
        logger.error(json.dumps({"context": "gsi.update", "error": f"AWS Error: {e.response['Error']['Code']} - {e.response['Error']['Message']}"}))
        sys.exit(1)

def update_billing_mode(dynamodb, table_name, mode, read=None, write=None, gsi_config=None, max_read_units=None, max_write_units=None):
    try:
        updated = False
        table_desc = dynamodb.describe_table(TableName=table_name)["Table"]
        current_mode = table_desc.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')
        existing_gsis = table_desc.get("GlobalSecondaryIndexes", [])

        if mode.upper() == "PROVISIONED":
            if current_mode == "PAY_PER_REQUEST":
                logger.info("Switching table to PROVISIONED mode...", extra={"context": "update_billing_mode"})
                update_kwargs = {
                    'TableName': table_name,
                    'BillingMode': 'PROVISIONED',
                    'ProvisionedThroughput': {
                        'ReadCapacityUnits': read or 5,
                        'WriteCapacityUnits': write or 5
                    }
                }
                if existing_gsis:
                    gsi_updates = []
                    for gsi in existing_gsis:
                        gsi_name = gsi["IndexName"]
                        gsi_throughput = gsi_config.get(gsi_name, {}).get("ProvisionedThroughput", {
                            "ReadCapacityUnits": 5,
                            "WriteCapacityUnits": 5
                        })
                        gsi_updates.append({
                            'Update': {
                                'IndexName': gsi_name,
                                'ProvisionedThroughput': {
                                    'ReadCapacityUnits': gsi_throughput.get("ReadCapacityUnits", 5),
                                    'WriteCapacityUnits': gsi_throughput.get("WriteCapacityUnits", 5)
                                }
                            }
                        })
                    update_kwargs["GlobalSecondaryIndexUpdates"] = gsi_updates
                    logger.info(json.dumps({
                        "context": "update_billing_mode",
                        "message": f"Including GSI updates for PROVISIONED mode: {gsi_updates}"
                    }))
                dynamodb.update_table(**update_kwargs)
                updated = True
                logger.info("Table and GSIs switched to PROVISIONED.", extra={"context": "update_billing_mode"})
                wait_for_table_active(dynamodb, table_name, target_billing_mode="PROVISIONED")
            else:
                logger.info("Table already in PROVISIONED mode. Checking GSI configuration...", extra={"context": "gsi.update"})
                if gsi_config:
                    updated = update_gsi_configuration(dynamodb, table_name, gsi_config) or updated
            if max_read_units is not None or max_write_units is not None:
                logger.warning(json.dumps({
                    "context": "update_billing_mode",
                    "message": "maxReadUnits and maxWriteUnits are ignored in PROVISIONED mode."
                }))
        else:
            if current_mode == "PROVISIONED":
                logger.info("Switching table to PAY_PER_REQUEST mode...", extra={"context": "update_billing_mode"})
                update_kwargs = {
                    'TableName': table_name,
                    'BillingMode': 'PAY_PER_REQUEST'
                }
                if max_read_units is not None or max_write_units is not None:
                    update_kwargs['OnDemandThroughput'] = {}
                    if max_read_units is not None:
                        logger.info(json.dumps({
                            "context": "update_billing_mode",
                            "message": f"Setting maximum read request units to {max_read_units}"
                        }))
                        update_kwargs['OnDemandThroughput']['MaxReadRequestUnits'] = max_read_units
                    if max_write_units is not None:
                        logger.info(json.dumps({
                            "context": "update_billing_mode",
                            "message": f"Setting maximum write request units to {max_write_units}"
                        }))
                        update_kwargs['OnDemandThroughput']['MaxWriteRequestUnits'] = max_write_units
                    logger.info(json.dumps({
                        "context": "update_billing_mode",
                        "message": f"OnDemandThroughput parameter added to update request: {update_kwargs['OnDemandThroughput']}"
                    }))
                dynamodb.update_table(**update_kwargs)
                updated = True
                wait_for_table_active(dynamodb, table_name, target_billing_mode="PAY_PER_REQUEST")
            else:
                if max_read_units is not None or max_write_units is not None:
                    logger.info("Table already in PAY_PER_REQUEST mode. Updating OnDemandThroughput...", extra={"context": "update_billing_mode"})
                    update_kwargs = {
                        'TableName': table_name,
                        'BillingMode': 'PAY_PER_REQUEST',
                        'OnDemandThroughput': {}
                    }
                    if max_read_units is not None:
                        logger.info(json.dumps({
                            "context": "update_billing_mode",
                            "message": f"Setting maximum read request units to {max_read_units}"
                        }))
                        update_kwargs['OnDemandThroughput']['MaxReadRequestUnits'] = max_read_units
                    if max_write_units is not None:
                        logger.info(json.dumps({
                            "context": "update_billing_mode",
                            "message": f"Setting maximum write request units to {max_write_units}"
                        }))
                        update_kwargs['OnDemandThroughput']['MaxWriteRequestUnits'] = max_write_units
                    logger.info(json.dumps({
                        "context": "update_billing_mode",
                        "message": f"OnDemandThroughput parameter added to update request: {update_kwargs['OnDemandThroughput']}"
                    }))
                    dynamodb.update_table(**update_kwargs)
                    updated = True
                    wait_for_table_active(dynamodb, table_name, target_billing_mode="PAY_PER_REQUEST")
                else:
                    logger.info("Table already in PAY_PER_REQUEST mode with no OnDemandThroughput changes.", extra={"context": "update_billing_mode"})
        if not updated:
            logger.info("No updates found for table or GSIs.", extra={"context": "update_billing_mode"})
            return
        final_desc = dynamodb.describe_table(TableName=table_name)["Table"]
        logger.info(json.dumps({
            "context": "table.verification",
            "message": f"Final table configuration in region {dynamodb.meta.region_name}",
            "TableName": table_name,
            "BillingMode": final_desc.get("BillingModeSummary", {}).get("BillingMode", "PROVISIONED"),
            "ReadCapacityUnits": final_desc.get("ProvisionedThroughput", {}).get("ReadCapacityUnits", 0),
            "WriteCapacityUnits": final_desc.get("ProvisionedThroughput", {}).get("WriteCapacityUnits", 0),
            "MaxReadRequestUnits": final_desc.get("OnDemandThroughput", {}).get("MaxReadRequestUnits", "Not set"),
            "MaxWriteRequestUnits": final_desc.get("OnDemandThroughput", {}).get("MaxWriteRequestUnits", "Not set")
        }))
    except ClientError as e:
        logger.error(json.dumps({"context": "update_billing_mode", "error": f"AWS Error: {e.response['Error']['Code']} - {e.response['Error']['Message']}"}))
        sys.exit(1)

def register_scaling(autoscaling, table_name, dimension, min_capacity, max_capacity, target_utilization, index_name=None):
    if dimension.startswith("dynamodb:table:"):
        resource_id = f"table/{table_name}"
    elif index_name:
        resource_id = f"table/{table_name}/index/{index_name}"
    else:
        logger.error(json.dumps({
            "context": f"autoscaling.register.{dimension}",
            "error": "Index name is required for GSI autoscaling"
        }))
        sys.exit(1)
    policy_name = f"{resource_id.replace('/', '-')}-{dimension.split(':')[-1]}-policy"
    try:
        if min_capacity < 1:
            logger.error(json.dumps({
                "context": f"autoscaling.register.{dimension}",
                "error": f"MinCapacity must be >= 1, got {min_capacity}"
            }))
            sys.exit(1)
        if max_capacity < min_capacity:
            logger.error(json.dumps({
                "context": f"autoscaling.register.{dimension}",
                "error": f"MaxCapacity must be >= MinCapacity, got max={max_capacity}, min={min_capacity}"
            }))
            sys.exit(1)
        if target_utilization <= 0 or target_utilization > 100:
            logger.error(json.dumps({
                "context": f"autoscaling.register.{dimension}",
                "error": f"TargetUtilization must be between 1 and 100, got {target_utilization}"
            }))
            sys.exit(1)
        logger.info(json.dumps({
            "context": f"autoscaling.register.{dimension}",
            "message": f"Parameters for scalable target: min={min_capacity}, max={max_capacity}, target={target_utilization}"
        }))
        try:
            autoscaling.deregister_scalable_target(
                ServiceNamespace="dynamodb",
                ResourceId=resource_id,
                ScalableDimension=dimension
            )
            logger.info(json.dumps({
                "context": f"autoscaling.cleanup.{dimension}",
                "message": f"Deregistered existing scalable target for {resource_id}"
            }))
        except ClientError as e:
            if e.response['Error']['Code'] != 'ObjectNotFoundException':
                logger.warning(json.dumps({
                    "context": f"autoscaling.cleanup.{dimension}",
                    "warning": f"Failed to deregister existing scalable target: {e.response['Error']['Code']} - {e.response['Error']['Message']}"
                }))
        scalable_targets = autoscaling.describe_scalable_targets(
            ServiceNamespace='dynamodb',
            ResourceIds=[resource_id],
            ScalableDimension=dimension
        )['ScalableTargets']
        existing_policies = autoscaling.describe_scaling_policies(
            ServiceNamespace='dynamodb',
            ResourceId=resource_id,
            ScalableDimension=dimension
        )['ScalingPolicies']
        policy_matches = False
        if scalable_targets and existing_policies:
            target = scalable_targets[0]
            policy = existing_policies[0]
            current_min = target.get('MinCapacity', 0)
            current_max = target.get('MaxCapacity', 0)
            current_target_value = policy.get('TargetTrackingScalingPolicyConfiguration', {}).get('TargetValue', 0)
            if (current_min == min_capacity and
                current_max == max_capacity and
                current_target_value == target_utilization and
                policy.get('PolicyType') == 'TargetTrackingScaling'):
                policy_matches = True
                logger.info(json.dumps({
                    "context": f"autoscaling.register.{dimension}",
                    "message": f"No updates needed for autoscaling policy: {policy_name}, settings already match (min={min_capacity}, max={max_capacity}, target={target_utilization})"
                }))
                return
        autoscaling.register_scalable_target(
            ServiceNamespace="dynamodb",
            ResourceId=resource_id,
            ScalableDimension=dimension,
            MinCapacity=min_capacity,
            MaxCapacity=max_capacity
        )
        for pol in existing_policies:
            logger.info(json.dumps({
                "context": f"autoscaling.cleanup.{dimension}",
                "message": f"Deleting existing policy: {pol['PolicyName']}"
            }))
            autoscaling.delete_scaling_policy(
                PolicyName=pol['PolicyName'],
                ServiceNamespace='dynamodb',
                ResourceId=resource_id,
                ScalableDimension=dimension
            )
        autoscaling.put_scaling_policy(
            PolicyName=policy_name,
            ServiceNamespace="dynamodb",
            ResourceId=resource_id,
            ScalableDimension=dimension,
            PolicyType='TargetTrackingScaling',
            TargetTrackingScalingPolicyConfiguration={
                'TargetValue': float(target_utilization),
                'PredefinedMetricSpecification': {
                    'PredefinedMetricType': 'DynamoDBReadCapacityUtilization'
                    if "Read" in dimension else 'DynamoDBWriteCapacityUtilization'
                },
                'ScaleInCooldown': 60,
                'ScaleOutCooldown': 60
            }
        )
        logger.info(json.dumps({
            "context": f"autoscaling.register.{dimension}",
            "message": "Scaling policy registered successfully"
        }))
        scalable_targets = autoscaling.describe_scalable_targets(
            ServiceNamespace='dynamodb',
            ResourceIds=[resource_id],
            ScalableDimension=dimension
        )['ScalableTargets']
        if scalable_targets:
            target = scalable_targets[0]
            logger.info(json.dumps({
                "context": f"autoscaling.verification.{dimension}",
                "message": f"Verified autoscaling settings for {resource_id}",
                "MinCapacity": target.get('MinCapacity'),
                "MaxCapacity": target.get('MaxCapacity')
            }))
        policies = autoscaling.describe_scaling_policies(
            ServiceNamespace='dynamodb',
            ResourceId=resource_id,
            ScalableDimension=dimension
        )['ScalingPolicies']
        if policies:
            policy = policies[0]
            logger.info(json.dumps({
                "context": f"autoscaling.verification.{dimension}",
                "message": f"Verified scaling policy for {resource_id}",
                "TargetValue": policy.get('TargetTrackingScalingPolicyConfiguration', {}).get('TargetValue')
            }))
    except ClientError as e:
        logger.error(json.dumps({
            "context": f"autoscaling.register.{dimension}",
            "error": f"AWS Error: {e.response['Error']['Code']} - {e.response['Error']['Message']}"
        }))
        sys.exit(1)

def deregister_scaling(autoscaling, table_name, dimension):
    try:
        autoscaling.deregister_scalable_target(
            ServiceNamespace="dynamodb",
            ResourceId=f"table/{table_name}",
            ScalableDimension=dimension
        )
        logger.info(json.dumps({
            "context": f"autoscaling.deregister.{dimension}",
            "message": f"Deregistered scalable target for table/{table_name}"
        }))
    except ClientError as e:
        if e.response['Error']['Code'] != 'ObjectNotFoundException':
            logger.warning(json.dumps({
                "context": f"autoscaling.deregister.{dimension}",
                "warning": f"Failed to deregister: {e.response['Error']['Code']} - {e.response['Error']['Message']}"
            }))

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--tableName", required=True)
    parser.add_argument("--region", required=True)
    parser.add_argument("--readMode", choices=["ondemand", "provisioned"], required=True)
    parser.add_argument("--writeMode", choices=["ondemand", "provisioned"], required=True)
    parser.add_argument("--minRead", type=int)
    parser.add_argument("--maxRead", type=int)
    parser.add_argument("--targetRead", type=int)
    parser.add_argument("--minWrite", type=int)
    parser.add_argument("--maxWrite", type=int)
    parser.add_argument("--targetWrite", type=int)
    parser.add_argument("--maxReadUnits", type=int, help="Maximum read request units for PAY_PER_REQUEST mode")
    parser.add_argument("--maxWriteUnits", type=int, help="Maximum write request units for PAY_PER_REQUEST mode")
    parser.add_argument("--checkGSI", type=lambda x: x.lower() == 'true', default=True)
    parser.add_argument("--dry-run", action="store_true")
    args = parser.parse_args()
    if args.readMode == "provisioned":
        if args.minRead is None or args.minRead < 1:
            logger.error(json.dumps({
                "context": "args.validation",
                "error": f"minRead must be >= 1 for PROVISIONED mode, got {args.minRead}"
            }))
            sys.exit(1)
        if args.maxRead is None or args.maxRead < args.minRead:
            logger.error(json.dumps({
                "context": "args.validation",
                "error": f"maxRead must be >= minRead, got maxRead={args.maxRead}, minRead={args.minRead}"
            }))
            sys.exit(1)
        if args.targetRead is None or args.targetRead <= 0 or args.targetRead > 100:
            logger.error(json.dumps({
                "context": "args.validation",
                "error": f"targetRead must be between 1 and 100, got {args.targetRead}"
            }))
            sys.exit(1)
    if args.writeMode == "provisioned":
        if args.minWrite is None or args.minWrite < 1:
            logger.error(json.dumps({
                "context": "args.validation",
                "error": f"minWrite must be >= 1 for PROVISIONED mode, got {args.minWrite}"
            }))
            sys.exit(1)
        if args.maxWrite is None or args.maxWrite < args.minWrite:
            logger.error(json.dumps({
                "context": "args.validation",
                "error": f"maxWrite must be >= minWrite, got maxWrite={args.maxWrite}, minWrite={args.minWrite}"
            }))
            sys.exit(1)
        if args.targetWrite is None or args.targetWrite <= 0 or args.targetWrite > 100:
            logger.error(json.dumps({
                "context": "args.validation",
                "error": f"targetWrite must be between 1 and 100, got {args.targetWrite}"
            }))
            sys.exit(1)
    logger.info(json.dumps({
        "context": "args",
        "message": f"Received arguments: tableName={args.tableName}, region={args.region}, readMode={args.readMode}, writeMode={args.writeMode}, minRead={args.minRead}, maxRead={args.maxRead}, targetRead={args.targetRead}, minWrite={args.minWrite}, maxWrite={args.maxWrite}, targetWrite={args.targetWrite}, maxReadUnits={args.maxReadUnits}, maxWriteUnits={args.maxWriteUnits}, checkGSI={args.checkGSI}, dry-run={args.dry_run}"
    }))
    dynamodb = boto3.client("dynamodb", region_name=args.region)
    autoscaling = boto3.client("application-autoscaling", region_name=args.region)
    logger.info("Found credentials from IAM Role: vpn-access", extra={"context": "autoscaling"})
    desc = get_table_description(dynamodb, args.tableName)
    current_mode = desc.get('BillingModeSummary', {}).get('BillingMode', 'PROVISIONED')
    gsi_config = load_gsi_config(args.tableName, args.checkGSI)
    validate_gsi_config(desc, gsi_config, args.checkGSI)
    if args.dry_run:
        logger.info(json.dumps({
            "context": "dry_run",
            "message": "Dry run mode - no changes applied",
            "tableName": args.tableName,
            "targetBilling": args.readMode,
            "checkGSI": args.checkGSI,
            "maxReadUnits": args.maxReadUnits,
            "maxWriteUnits": args.maxWriteUnits
        }))
        sys.exit(0)
    replica_regions = get_replica_regions(dynamodb, args.tableName)
    logger.info(json.dumps({
        "context": "global_tables",
        "message": f"Found replica regions: {replica_regions}"
    }))
    if args.readMode == "provisioned" or args.writeMode == "provisioned":
        if current_mode == "PAY_PER_REQUEST":
            deregister_scaling(autoscaling, args.tableName, "dynamodb:table:ReadCapacityUnits")
            deregister_scaling(autoscaling, args.tableName, "dynamodb:table:WriteCapacityUnits")
        update_billing_mode(dynamodb, args.tableName, "provisioned",
                           read=args.minRead, write=args.minWrite,
                           gsi_config=gsi_config,
                           max_read_units=args.maxReadUnits,
                           max_write_units=args.maxWriteUnits)
        wait_for_table_active(dynamodb, args.tableName, target_billing_mode="PROVISIONED")
        if args.readMode == "provisioned":
            register_scaling(autoscaling, args.tableName, "dynamodb:table:ReadCapacityUnits",
                            args.minRead, args.maxRead, args.targetRead)
        if args.writeMode == "provisioned":
            register_scaling(autoscaling, args.tableName, "dynamodb:table:WriteCapacityUnits",
                            args.minWrite, args.maxWrite, args.targetWrite)
        if args.checkGSI and gsi_config:
            for index_name, conf in gsi_config.items():
                logger.info(json.dumps({
                    "context": "autoscaling.config",
                    "message": f"Applying autoscaling for index: {index_name}"
                }))
                if "MinReadCapacityUnits" in conf:
                    register_scaling(
                        autoscaling,
                        args.tableName,
                        "dynamodb:index:ReadCapacityUnits",
                        conf["MinReadCapacityUnits"],
                        conf["MaxReadCapacityUnits"],
                        conf["TargetReadUtilization"],
                        index_name=index_name
                    )
                if "MinWriteCapacityUnits" in conf:
                    register_scaling(
                        autoscaling,
                        args.tableName,
                        "dynamodb:index:WriteCapacityUnits",
                        conf["MinWriteCapacityUnits"],
                        conf["MaxWriteCapacityUnits"],
                        conf["TargetWriteUtilization"],
                        index_name=index_name
                    )
        logger.info(json.dumps({
            "context": "pipeline.verification",
            "message": f"Pipeline completed successfully for table {args.tableName} in region {args.region}"
        }))
        sys.exit(0)  # Explicit success exit code
    else:
        if current_mode == "PROVISIONED":
            deregister_scaling(autoscaling, args.tableName, "dynamodb:table:ReadCapacityUnits")
            deregister_scaling(autoscaling, args.tableName, "dynamodb:table:WriteCapacityUnits")
        update_billing_mode(dynamodb, args.tableName, "ondemand",
                           max_read_units=args.maxReadUnits, max_write_units=args.maxWriteUnits)
        wait_for_table_active(dynamodb, args.tableName, target_billing_mode="PAY_PER_REQUEST")
        logger.info(json.dumps({
            "context": "pipeline.verification",
            "message": f"Pipeline completed successfully for table {args.tableName} in region {args.region}"
        }))
        sys.exit(0)  # Explicit success exit code
    all_regions = [args.region] + replica_regions
    for region in all_regions:
        region_dynamodb = boto3.client("dynamodb", region_name=region)
        final_desc = get_table_description(region_dynamodb, args.tableName)
        logger.info(json.dumps({
            "context": "table.verification",
            "message": f"Final table configuration in region {region}",
            "TableName": args.tableName,
            "BillingMode": final_desc.get("BillingModeSummary", {}).get("BillingMode", "PROVISIONED"),
            "ReadCapacityUnits": final_desc.get("ProvisionedThroughput", {}).get("ReadCapacityUnits", 0),
            "WriteCapacityUnits": final_desc.get("ProvisionedThroughput", {}).get("WriteCapacityUnits", 0),
            "MaxReadRequestUnits": final_desc.get("OnDemandThroughput", {}).get("MaxReadRequestUnits", "Not set"),
            "MaxWriteRequestUnits": final_desc.get("OnDemandThroughput", {}).get("MaxWriteRequestUnits", "Not set")
        }))

if __name__ == "__main__":
    main()
