vents:
Type Reason Age From Message
---- ------ ---- ---- -------
Normal Scheduled 43m default-scheduler Successfully assigned prod/k811-finbridge-connector-55b7658484-qrtx6 to ip-10-59-84-228.ap-south-1.compute.internal
Normal Pulling 41m (x4 over 43m) kubelet Pulling image "718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_istio_proxy:stable-"
Warning Failed 41m (x4 over 43m) kubelet Failed to pull image "718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_istio_proxy:stable-": rpc error: code = NotFound desc = failed to pull and unpack image "718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_istio_proxy:stable-": failed to resolve reference "718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_istio_proxy:stable-": 718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_istio_proxy:stable-: not found
Warning Failed 41m (x4 over 43m) kubelet Error: ErrImagePull
Warning Failed 41m (x6 over 43m) kubelet Error: ImagePullBackOff
Normal BackOff 3m2s (x176 over 43m) kubelet Back-off pulling image "718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_istio_proxy:stable-"
app_user@KBPREC81L10281:~$ docker image -ls

below is yaml
pipline
trigger: none

# pass varibale to build.yaml file.
variables:
- group: Kotak-ECR-Credentials
# - name: SERVICE_NAME
- name: ENV
value: prod
- name: DR-ENV
value: prod-dr
# For AWS Role Access
- name: PROD_ROLE_NAME
value: EKS-Setup-Role-Kotak811
- name: UAT_ROLE_NAME
value: EKS_Setup_Role
# details for prod repo to update pro-values.yaml
- name: BRANCH_NAME
value: master
- name: REPOSITORY_NAME
value: kotak-811-devops-prod

# Agent pool details
- name: DEVOPS_POOL_NAME
value: "K811-DevOps"
- name: PROD_POOL_NAME
value: k811-prod
- name: PROD_AGENT_IN_AGENTPOOL
value: Linux-Agent3
# docker details to push to prod ECR
- name: PROD_PREFIX
value: stable
- name: PROD_AWS_REGION
value: ap-south-1
- name: PROD_AWS_ACCOUNT_ID
value: "718378052708"
- name: PROD_ECR_FOLDER_NAME
value: 811prodonb
- name: PROD_ECR_REPO_NAME
value: "finbridge-connector"


# source Enviornmnet details
# - name: SOURCE_SERVICE_NAME
- name: SOURCE_AWS_REGION
value: ap-south-1
- name: SOURCE_AWS_ACCOUNT_ID
value: "483584640083"
- name: SOURCE_ECR_FOLDER_NAME
value: 811uatonb
- name: SOURCE_ECR_REPO_NAME
value: "k811_app_ms_finbridge_connector"
- name: SOURCE_KUBE_CONFIG_PATH
value: /home/app_user/.kube/config-uat
- name: SOURCE_NAMESPACE
value: 811-uat
# helm related variables
- name: HELM_CHARTS_PATH
value: helm-charts/${{ parameters.service }}/charts
- name: HELM_S3BUCKET_URL
value: s3://k811-onb-helmcharts/prod/${{ parameters.service }}/
- name: DR_HELM_S3BUCKET_URL
value: s3://k811-onb-helmcharts-dr/prod/${{ parameters.service }}/
- name: KUBE_CONFIG_PATH
value: /home/app_user/.kube/config-prod
- name: DR_KUBE_CONFIG_PATH
value: /home/app_user/.kube/config-onb-dr
- name: NAMESPACE
value: prod
- name: ECR_ISTIO_REPO_NAME
value: k811_ms_istio_proxy

# Parameter to input Jira Ticket
parameters:
- name: Jira_URL
type: string
default: ""
displayName: "Jira URL"
- name: service
type: string
displayName: 'Select the Service Name for Deploy'
default: k811-finbridge-connector
values:
- k811-finbridge-connector
# - name: image_tag
# type: string
# default: ""
# displayName: "Image Tag"
# - name: branch_name
# type: string
# default: master
# displayName: "Pipeline repository branch"

stages:
- stage: ECR_CLONE
pool:
name: $(DEVOPS_POOL_NAME)
jobs:
- deployment: PRODDeployment
displayName: PRODDeployment
environment: prod-pipeline-approvals
strategy:
runOnce:
deploy:
steps:
- script: |
echo "approved to deploy UAT Image to PROD"
displayName: Approve UAT image to depoly in PROD
- job: ECR_IMAGE_PUSH_FROM_UAT_TO_PROD
steps:
- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(SOURCE_AWS_ACCOUNT_ID):role/$(UAT_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
version=$(kubectl get deployment ${{ parameters.service }} -o jsonpath=''{$.spec.template.spec.containers[:1].image}'' -n $(SOURCE_NAMESPACE) --kubeconfig $(SOURCE_KUBE_CONFIG_PATH) | cut -d ':' -f 2)
echo "##vso[task.setvariable variable=version]$version"
istio_image=$(kubectl get deployment ${{ parameters.service }} -o jsonpath='{.spec.template.metadata.annotations.sidecar\.istio\.io/proxyImage}' -n $(SOURCE_NAMESPACE) --kubeconfig $(SOURCE_KUBE_CONFIG_PATH) | awk -F':' '{print $2}')
echo "##vso[task.setvariable variable=istio_image]$istio_image"
displayName: "${{ parameters.service }} Tag value in UAT environment ."

- script: |
echo $(istio_image)
echo $(version)
displayName: UAT Environment Docker image of ${{ parameters.service }}

- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(SOURCE_AWS_ACCOUNT_ID):role/$(UAT_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
aws ecr get-login-password --region $(SOURCE_AWS_REGION) | docker login --username AWS --password-stdin $(SOURCE_AWS_ACCOUNT_ID).dkr.ecr.$(SOURCE_AWS_REGION).amazonaws.com
docker pull $(SOURCE_AWS_ACCOUNT_ID).dkr.ecr.$(SOURCE_AWS_REGION).amazonaws.com/$(SOURCE_ECR_FOLDER_NAME)/$(SOURCE_ECR_REPO_NAME):$(version)
docker tag $(SOURCE_AWS_ACCOUNT_ID).dkr.ecr.$(SOURCE_AWS_REGION).amazonaws.com/$(SOURCE_ECR_FOLDER_NAME)/$(SOURCE_ECR_REPO_NAME):$(version) $(PROD_AWS_ACCOUNT_ID).dkr.ecr.$(PROD_AWS_REGION).amazonaws.com/$(PROD_ECR_FOLDER_NAME)/$(PROD_ECR_REPO_NAME):$(PROD_PREFIX)-$(version)
aws ecr get-login-password --region $(PROD_AWS_REGION) | docker login --username AWS --password-stdin $(PROD_AWS_ACCOUNT_ID).dkr.ecr.$(PROD_AWS_REGION).amazonaws.com
docker push $(PROD_AWS_ACCOUNT_ID).dkr.ecr.$(PROD_AWS_REGION).amazonaws.com/$(PROD_ECR_FOLDER_NAME)/$(PROD_ECR_REPO_NAME):$(PROD_PREFIX)-$(version)
# ---Istio Block ---
if [ -n "$(istio_image)" ]; then
echo "Istio sidecar image detected: $(istio_image). Pulling/tagging/pushing..."
docker pull $(SOURCE_AWS_ACCOUNT_ID).dkr.ecr.$(SOURCE_AWS_REGION).amazonaws.com/$(SOURCE_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(istio_image)
docker tag $(SOURCE_AWS_ACCOUNT_ID).dkr.ecr.$(SOURCE_AWS_REGION).amazonaws.com/$(SOURCE_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(istio_image) $(PROD_AWS_ACCOUNT_ID).dkr.ecr.$(PROD_AWS_REGION).amazonaws.com/$(PROD_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(PROD_PREFIX)-$(istio_image)
docker push $(PROD_AWS_ACCOUNT_ID).dkr.ecr.$(PROD_AWS_REGION).amazonaws.com/$(PROD_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(PROD_PREFIX)-$(istio_image)
else
echo "No Istio sidecar image for this service. Skipping Istio image pull/tag/push."
fi
# --- End Istio ---
# docker pull $(SOURCE_AWS_ACCOUNT_ID).dkr.ecr.$(SOURCE_AWS_REGION).amazonaws.com/$(SOURCE_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(istio_image)
# docker tag $(SOURCE_AWS_ACCOUNT_ID).dkr.ecr.$(SOURCE_AWS_REGION).amazonaws.com/$(SOURCE_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(istio_image) $(PROD_AWS_ACCOUNT_ID).dkr.ecr.$(PROD_AWS_REGION).amazonaws.com/$(PROD_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(PROD_PREFIX)-$(istio_image)
# docker push $(PROD_AWS_ACCOUNT_ID).dkr.ecr.$(PROD_AWS_REGION).amazonaws.com/$(PROD_ECR_FOLDER_NAME)/$(ECR_ISTIO_REPO_NAME):$(PROD_PREFIX)-$(istio_image)
displayName: "Login, Docker and Push"

- script: |
git clone https://$(PAT)@kmbl-devops.visualstudio.com/Kotak%20811%20Onboarding%20App/_git/$(REPOSITORY_NAME) -b $(BRANCH_NAME)
displayName: "clone repository"

- script: |
pwd
git pull
sed -i -e 's/tag:.*/tag: $(PROD_PREFIX)-$(version)/' $(HELM_CHARTS_PATH)/$(ENV)-values.yaml
sed -i -e 's/tag:.*/tag: $(PROD_PREFIX)-$(version)/' $(HELM_CHARTS_PATH)/$(DR-ENV)-values.yaml
sed -i -e 's/istio_tag:.*/istio_tag: $(PROD_PREFIX)-$(istio_image)/' $(HELM_CHARTS_PATH)/$(ENV)-values.yaml
sed -i -e 's/istio_tag:.*/istio_tag: $(PROD_PREFIX)-$(istio_image)/' $(HELM_CHARTS_PATH)/$(DR-ENV)-values.yaml
displayName: "replace helm charts"
workingDirectory: $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/

- script: |
git config --global user.email "pipeline@kotak.com"
git config --global user.name "Pipeline"
git add -A
git status
git commit -m "azure pipeline commited to updated prod-values.yaml in helm charts"
echo "Pushing now!!!"
git pull
git push -u origin $(BRANCH_NAME)
displayName: "Updating Chart Values"
workingDirectory: $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/

- stage: PROD
pool:
name: $(PROD_POOL_NAME)
# demands:
# - agent.name -equals $(PROD_AGENT_IN_AGENTPOOL)
displayName: "Build in PROD Environment"
jobs:
# UNCOMMENT HERE FOR PROD MUMBAI REGION DEPLOYMENT
- job: Service_deployment_in_PROD
displayName: "Service deployment in PROD"
steps:
- script: |
git clone https://$(PAT)@kmbl-devops.visualstudio.com/Kotak%20811%20Onboarding%20App/_git/$(REPOSITORY_NAME) -b $(BRANCH_NAME)
displayName: "clone repository"
- script: |
pwd
sed -i -e 's/name:.*/name: ${{ parameters.service }}/' $(HELM_CHARTS_PATH)/Chart.yaml
sed -i -e 's/appVersion:.*/appVersion: 1.$(Build.BuildNumber)/' $(HELM_CHARTS_PATH)/Chart.yaml
sed -i -e 's/version:.*/version: 1.$(Build.BuildNumber)/' $(HELM_CHARTS_PATH)/Chart.yaml
displayName: "replace helm charts"
workingDirectory: $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/

- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(PROD_AWS_ACCOUNT_ID):role/$(PROD_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
helm s3 init $(HELM_S3BUCKET_URL)
helm repo add $(ENV)-${{ parameters.service }} $(HELM_S3BUCKET_URL)
helm package $(HELM_CHARTS_PATH)/
helm s3 push --force ${{ parameters.service }}-1.$(Build.BuildNumber).tgz $(ENV)-${{ parameters.service }}
aws s3 ls $(HELM_S3BUCKET_URL)
helm repo update
helm search repo $(ENV)-${{ parameters.service }}
displayName: "Charts Push To S3"
workingDirectory: $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/

- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(PROD_AWS_ACCOUNT_ID):role/$(PROD_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
rm -rf $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/*.tgz
helm repo update
helm fetch $(ENV)-${{ parameters.service }}/${{ parameters.service }}
tar -xvf ${{ parameters.service }}-1.$(Build.BuildNumber).tgz
helm upgrade --install $(ENV)-${{ parameters.service }} ${{ parameters.service }}/ -f ${{ parameters.service }}/$(ENV)-values.yaml --namespace $(NAMESPACE) --kubeconfig $(KUBE_CONFIG_PATH) --wait --timeout 3m
displayName: "Helm Charts Deployment"

- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(PROD_AWS_ACCOUNT_ID):role/$(PROD_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
sleep 10
echo "########################## list of pods ########################################"
kubectl get po --namespace $(NAMESPACE) --kubeconfig $(KUBE_CONFIG_PATH) | grep -i ${{ parameters.service }}
echo "########################## list of deployed services ########################################"
kubectl get deployment --namespace $(NAMESPACE) --kubeconfig $(KUBE_CONFIG_PATH) | grep -i ${{ parameters.service }}
displayName: "Application Status"

- job: Service_deployment_in_PROD_DR
displayName: "Service deployment in PROD DR"
steps:
- script: |
git clone https://$(PAT)@kmbl-devops.visualstudio.com/Kotak%20811%20Onboarding%20App/_git/$(REPOSITORY_NAME) -b $(BRANCH_NAME)
displayName: "clone repository"
- script: |
pwd
sed -i -e 's/name:.*/name: ${{ parameters.service }}/' $(HELM_CHARTS_PATH)/Chart.yaml
sed -i -e 's/appVersion:.*/appVersion: 1.$(Build.BuildNumber)/' $(HELM_CHARTS_PATH)/Chart.yaml
sed -i -e 's/version:.*/version: 1.$(Build.BuildNumber)/' $(HELM_CHARTS_PATH)/Chart.yaml
displayName: "replace helm charts"
workingDirectory: $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/

- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(PROD_AWS_ACCOUNT_ID):role/$(PROD_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
helm s3 init $(DR_HELM_S3BUCKET_URL)
helm repo add $(DR-ENV)-${{ parameters.service }} $(DR_HELM_S3BUCKET_URL)
helm package $(HELM_CHARTS_PATH)/
helm s3 push --force ${{ parameters.service }}-1.$(Build.BuildNumber).tgz $(DR-ENV)-${{ parameters.service }}
aws s3 ls $(DR_HELM_S3BUCKET_URL)
helm repo update
helm search repo $(DR-ENV)-${{ parameters.service }}
displayName: "Charts Push To S3"
workingDirectory: $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/

- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(PROD_AWS_ACCOUNT_ID):role/$(PROD_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
rm -rf $(System.DefaultWorkingDirectory)/$(REPOSITORY_NAME)/*.tgz
helm repo update
helm fetch $(DR-ENV)-${{ parameters.service }}/${{ parameters.service }}
tar -xvf ${{ parameters.service }}-1.$(Build.BuildNumber).tgz
helm upgrade --install $(DR-ENV)-${{ parameters.service }} ${{ parameters.service }}/ -f ${{ parameters.service }}/$(DR-ENV)-values.yaml --namespace $(NAMESPACE) --kubeconfig $(DR_KUBE_CONFIG_PATH) --wait --timeout 3m
displayName: "Helm Charts Deployment"

- script: |
pwd
unset AWS_SESSION_TOKEN
unset AWS_SECRET_ACCESS_KEY
unset AWS_ACCESS_KEY_ID
CREDENTIALS=`aws sts assume-role --role-arn arn:aws:iam::$(PROD_AWS_ACCOUNT_ID):role/$(PROD_ROLE_NAME) --role-session-name $(Build.DefinitionName)-$(Build.BuildNumber)`
export AWS_SESSION_TOKEN=`echo $CREDENTIALS | jq -r '.Credentials.SessionToken'`
export AWS_SECRET_ACCESS_KEY=`echo $CREDENTIALS | jq -r '.Credentials.SecretAccessKey'`
export AWS_ACCESS_KEY_ID=`echo $CREDENTIALS | jq -r '.Credentials.AccessKeyId'`
sleep 10
echo "########################## list of pods ########################################"
kubectl get po --namespace $(NAMESPACE) --kubeconfig $(DR_KUBE_CONFIG_PATH) | grep -i ${{ parameters.service }}
echo "########################## list of deployed services ########################################"
kubectl get deployment --namespace $(NAMESPACE) --kubeconfig $(DR_KUBE_CONFIG_PATH) | grep -i ${{ parameters.service }}
displayName: "Application Status"

below is value.yaml
namespace: prod
env: "prod"
deadlineSeconds: 600
autoscaling:
minReplicas: 2
maxReplicas: 8
targetCPUUtilizationPercentage: 50
targetMemoryUtilizationPercentage: 50
hpa:
enabled: true
PodDisruptionBudget:
maxUnavailable: 1
trafficip: 10.0.0.0/8
appconfig:
APPCONFIG_ENABLED: true
APPCONFIG_APPLICATION: 'onb-uscc-applications'
APPCONFIG_ENVIRONMENT: 'production'
APPCONFIG_PROFILE: 'uscc-applications-prod'
APPCONFIG_MSK_SECRETS_PROFILE: 'uscc-application-msk-secret-prod'
APPCONFIG_RDS_SECRETS_PROFILE: 'uscc-applications-secret-prod'
APPCONFIG_MAX_CONFIG_AGE: 86400
# APPCONFIG_FEATURE_ENABLED: true
# APPCONFIG_FEATURE_FLAG_APPLICATION: 'onb-uscc-applications'
# APPCONFIG_FEATURE_FLAG_ENVIRONMENT: 'uat'
# APPCONFIG_FEATURE_FLAG_PROFILE: 'uat-uscc-applications-feature-flag'
# APPCONFIG_FEATURE_FLAG_MAX_CONFIG_AGE: 300
COM_APPCONFIG_APPLICATION: "811-services-appconfig"
COM_APPCONFIG_PROFILE: "common-appconfig-pord"
COM_APPCONFIG_MAX_CONFIG_AGE: "86400"
ENV: "prod"
PROJECT_NAME: "811_app_ms_uscc_applications"
OTEL_RESOURCE_ATTRIBUTES: "application=k811-onb-prod"
OTEL_SERVICE_NAME: "k811-ms-uscc-applications"
OTEL_METRICS_EXPORTER: "none"
OTEL_TRACES_EXPORTER: "otlp"
OTEL_EXPORTER_OTLP_ENDPOINT: "http://$(NODE):4318"
OTEL_PROPAGATORS: "b3,tracecontext,baggage"
OTEL_PYTHON_LOG_CORRELATION: "true"
OTEL_EXPORTER_OTLP_PROTOCOL: "http/protobuf"
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST: "x-platform-id,x-reg-id,x-request-id,x-session-id"
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE: "x-platform-id,x-reg-id,x-request-id,x-session-id"
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_CLIENT_REQUEST: "x-platform-id,x-reg-id,x-request-id,x-session-id"
OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_CLIENT_RESPONSE: "x-platform-id,x-reg-id,x-request-id,x-session-id"
node:
name: node
operator: In
nodegroup:
labels: ["stateless-spot,stateless-ondemand"]
probePath: /uscc-applications/health
image:
repository: 718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_uscc_applications
tag: stable-811_app_ms_uscc_applications-development-20250712.3
name: k811-uscc-applications
serviceaccount:
enabled: true
name: 811-onb-prod-sa-uscc-applications
role: arn:aws:iam::718378052708:role/k811-onb-prod-eks-uscc-applications-sa
rolebinding: k811-uscc-applications
service:
name: k811-uscc-applications
type: ClusterIP
externalPort: 8080
internalPort: 8080
portName: k811-uscc-applications
resources:
limits:
cpu: 1000m
memory: 2Gi
requests:
cpu: 500m
memory: 1Gi
gateways:
gateway: gateway
prefix: "/uscc-applications"
host: "*"
internalPort: 8080
weightvalue: 100
isCustomIstio:
enabled: true
encryption: "prod/istio_codec/symmetric20240618165134578300000001"
enc_path: /uscc-applications/v1/update-application|/uscc-applications/v1/cancel-application|/uscc-applications/v1/submit-application|/uscc-applications/v1/application/status
exclude_enc_path: '$^'
aws_region: "ap-south-1"
custom_istio_image:
repository: 718378052708.dkr.ecr.ap-south-1.amazonaws.com/811prodonb/k811_ms_istio_proxy
istio_tag: stable-custom-encoder-proxyv2.1.24.6-enhanced
spreadAz: true
podAntiAffinity: "none" #Accepted values are none, soft and hard. If this variable not defined then takes hard scheduling by default


what is reason?
